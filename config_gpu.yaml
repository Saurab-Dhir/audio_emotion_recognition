paths:
  raw_data: "./data/raw/"
  processed_data: "./data/processed/"
  features: "./data/features/"
  models: "./models/"
  results: "./results/"

dataset:
  cremad:
    path: "./data/raw/cremad/"
    sample_rate: 16000
    emotions:
      "ANG": "angry"
      "DIS": "disgust"
      "FEA": "fear"
      "HAP": "happy"
      "NEU": "neutral"
      "SAD": "sad"
    intensity:
      "LO": "low"
      "MD": "medium"
      "HI": "high"
      "XX": "unspecified"

preprocessing:
  normalize: true
  noise_reduction: true
  segment_length_ms: 3000
  hop_length_ms: 1000
  batch_size: 32  # Process data in batches to avoid memory issues

features:
  mfcc:
    n_mfcc: 13
    include_delta: true
    include_delta_delta: true
  prosodic:
    extract_pitch: true
    extract_energy: true
    extract_zero_crossing_rate: true
  spectral:
    extract_spectral_centroid: true
    extract_spectral_rolloff: true
    extract_spectral_flux: true
    extract_spectral_bandwidth: true

model:
  random_state: 42
  test_size: 0.2
  validation_size: 0.2
  models_to_train:
    - "xgboost"  # Start with just XGBoost since it has the best GPU support
  hyperparameter_tuning:
    perform: false  # Set to false initially to speed up testing
    cv_folds: 3     # Reduced from 5 to 3 to speed up training

evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"

gpu:
  enabled: true
  precision: "float32"  # Can be float32 or float16 for faster training
  memory_growth: true   # Enable memory growth to avoid taking all GPU memory
  batch_size: 64        # Larger batch size for GPU training