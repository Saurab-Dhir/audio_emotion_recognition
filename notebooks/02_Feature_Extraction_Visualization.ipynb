{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Feature Extraction and Visualization\n",
    "\n",
    "This notebook demonstrates the extraction and visualization of audio features for emotion recognition using the CREMA-D dataset.\n",
    "\n",
    "We'll explore:\n",
    "1. Audio preprocessing techniques\n",
    "2. Feature extraction methods\n",
    "3. Visualization of features across different emotions\n",
    "4. Feature distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Add the src directory to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils import load_config\n",
    "from src.cremad_loader import CREMADDataLoader\n",
    "from src.preprocessing import AudioPreprocessor\n",
    "from src.features import FeatureExtractor\n",
    "\n",
    "# Set some plotting parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 14:24:30,402 - src.cremad_loader - INFO - Found 0 audio files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing metadata: 0it [00:00, ?it/s]\n",
      "2025-04-09 14:24:30,405 - src.cremad_loader - WARNING - No metadata was extracted, check your dataset path and file format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 audio samples\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config.yaml')\n",
    "print(\"Configuration loaded successfully\")\n",
    "\n",
    "# Create data loader\n",
    "data_loader = CREMADDataLoader('../config.yaml')\n",
    "\n",
    "# Load a subset of the dataset (stratified by emotion)\n",
    "metadata_df, audio_data = data_loader.load_dataset(limit=60, stratify_by='emotion')\n",
    "\n",
    "print(f\"Loaded {len(metadata_df)} audio samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion distribution:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'emotion'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Display dataset statistics\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEmotion distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m emotion_counts = \u001b[43mmetadata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memotion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(emotion_counts)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIntensity distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ogsaurab\\Documents\\Github\\audio_emotion_recognition\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ogsaurab\\Documents\\Github\\audio_emotion_recognition\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'emotion'"
     ]
    }
   ],
   "source": [
    "# Display dataset statistics\n",
    "print(\"\\nEmotion distribution:\")\n",
    "emotion_counts = metadata_df['emotion'].value_counts()\n",
    "print(emotion_counts)\n",
    "\n",
    "print(\"\\nIntensity distribution:\")\n",
    "intensity_counts = metadata_df['intensity'].value_counts()\n",
    "print(intensity_counts)\n",
    "\n",
    "print(\"\\nGender distribution:\")\n",
    "gender_counts = metadata_df['gender'].value_counts()\n",
    "print(gender_counts)\n",
    "\n",
    "# Plot emotion distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=metadata_df, x='emotion', palette='viridis')\n",
    "plt.title('Emotion Distribution')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Audio Preprocessing\n",
    "\n",
    "Let's apply our preprocessing pipeline to one sample from each emotion and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = AudioPreprocessor(config_path=os.path.join('..', 'config.yaml'))\n",
    "\n",
    "# Get one sample from each emotion\n",
    "emotion_samples = {}\n",
    "for emotion in metadata_df['emotion'].unique():\n",
    "    sample_idx = metadata_df[metadata_df['emotion'] == emotion].index[0]\n",
    "    emotion_samples[emotion] = {\n",
    "        'metadata': metadata_df.loc[sample_idx],\n",
    "        'audio_data': audio_data[sample_idx]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess samples and visualize\n",
    "for emotion, sample in emotion_samples.items():\n",
    "    y, sr = sample['audio_data']\n",
    "    \n",
    "    # Preprocess audio\n",
    "    y_preprocessed = preprocessor.preprocess_audio(y, sr)\n",
    "    \n",
    "    # Plot original and preprocessed waveforms\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(f\"Original Waveform - {emotion.capitalize()}\")\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.waveshow(y_preprocessed, sr=sr)\n",
    "    plt.title(f\"Preprocessed Waveform - {emotion.capitalize()}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Listen to original and preprocessed audio\n",
    "    print(f\"Original audio - {emotion.capitalize()}:\")\n",
    "    display(Audio(y, rate=sr))\n",
    "    \n",
    "    print(f\"Preprocessed audio - {emotion.capitalize()}:\")\n",
    "    display(Audio(y_preprocessed, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Let's extract features from our samples and examine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(config_path=os.path.join('..', 'config.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize raw features for each emotion\n",
    "for emotion, sample in emotion_samples.items():\n",
    "    y, sr = sample['audio_data']\n",
    "    \n",
    "    # Preprocess audio\n",
    "    y_preprocessed = preprocessor.preprocess_audio(y, sr)\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor.extract_all_features(y_preprocessed, sr)\n",
    "    \n",
    "    # Plot MFCC features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    librosa.display.specshow(features['mfcc'][:13], x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"MFCC Features - {emotion.capitalize()}\")\n",
    "    \n",
    "    if features['mfcc'].shape[0] > 13:\n",
    "        plt.subplot(3, 1, 2)\n",
    "        librosa.display.specshow(features['mfcc'][13:26], x_axis='time')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"MFCC Delta Features - {emotion.capitalize()}\")\n",
    "        \n",
    "        plt.subplot(3, 1, 3)\n",
    "        librosa.display.specshow(features['mfcc'][26:], x_axis='time')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"MFCC Delta-Delta Features - {emotion.capitalize()}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot prosodic features\n",
    "    if features['prosodic'].size > 0:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i in range(features['prosodic'].shape[0]):\n",
    "            plt.plot(features['prosodic'][i], label=f\"Feature {i+1}\")\n",
    "        plt.title(f\"Prosodic Features - {emotion.capitalize()}\")\n",
    "        plt.xlabel(\"Time Frame\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot spectral features\n",
    "    if features['spectral'].size > 0:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i in range(features['spectral'].shape[0]):\n",
    "            plt.plot(features['spectral'][i], label=f\"Feature {i+1}\")\n",
    "        plt.title(f\"Spectral Features - {emotion.capitalize()}\")\n",
    "        plt.xlabel(\"Time Frame\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Feature Statistics\n",
    "\n",
    "Now let's extract statistical features from all our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all audio samples\n",
    "preprocessed_audio = []\n",
    "for y, sr in tqdm(audio_data, desc=\"Preprocessing audio\"):\n",
    "    y_preprocessed = preprocessor.preprocess_audio(y, sr)\n",
    "    preprocessed_audio.append((y_preprocessed, sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all preprocessed samples\n",
    "all_feature_vectors = []\n",
    "for y, sr in tqdm(preprocessed_audio, desc=\"Extracting features\"):\n",
    "    # Extract features\n",
    "    features = feature_extractor.extract_all_features(y, sr)\n",
    "    \n",
    "    # Compute statistics\n",
    "    statistics = feature_extractor.compute_feature_statistics(features)\n",
    "    \n",
    "    # Concatenate all statistics into a single feature vector\n",
    "    feature_vectors = []\n",
    "    for feature_name in sorted(statistics.keys()):\n",
    "        feature_vectors.append(statistics[feature_name])\n",
    "    \n",
    "    if feature_vectors:\n",
    "        feature_vector = np.concatenate(feature_vectors)\n",
    "        all_feature_vectors.append(feature_vector)\n",
    "    else:\n",
    "        all_feature_vectors.append(np.array([]))\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(all_feature_vectors)\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Visualization\n",
    "\n",
    "Let's visualize the features using dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame with PCA results and emotion labels\n",
    "pca_df = pd.DataFrame({\n",
    "    'pca1': X_pca[:, 0],\n",
    "    'pca2': X_pca[:, 1],\n",
    "    'emotion': metadata_df['emotion'].values,\n",
    "    'gender': metadata_df['gender'].values,\n",
    "    'intensity': metadata_df['intensity'].values\n",
    "})\n",
    "\n",
    "# Plot PCA results by emotion\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=pca_df, x='pca1', y='pca2', hue='emotion', palette='viridis', s=100)\n",
    "plt.title('PCA of Audio Features by Emotion')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for better visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame with t-SNE results and emotion labels\n",
    "tsne_df = pd.DataFrame({\n",
    "    'tsne1': X_tsne[:, 0],\n",
    "    'tsne2': X_tsne[:, 1],\n",
    "    'emotion': metadata_df['emotion'].values,\n",
    "    'gender': metadata_df['gender'].values,\n",
    "    'intensity': metadata_df['intensity'].values\n",
    "})\n",
    "\n",
    "# Plot t-SNE results by emotion\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=tsne_df, x='tsne1', y='tsne2', hue='emotion', palette='viridis', s=100)\n",
    "plt.title('t-SNE of Audio Features by Emotion')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE results by intensity\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=tsne_df, x='tsne1', y='tsne2', hue='intensity', palette='plasma', s=100)\n",
    "plt.title('t-SNE of Audio Features by Intensity')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE results by gender\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=tsne_df, x='tsne1', y='tsne2', hue='gender', palette='Set1', s=100)\n",
    "plt.title('t-SNE of Audio Features by Gender')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "Let's examine how the features contribute to the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with more components for analysis\n",
    "n_components = min(10, X_scaled.shape[1])\n",
    "pca_analysis = PCA(n_components=n_components)\n",
    "pca_analysis.fit(X_scaled)\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, n_components+1), pca_analysis.explained_variance_ratio_)\n",
    "plt.plot(range(1, n_components+1), np.cumsum(pca_analysis.explained_variance_ratio_), marker='o', linestyle='-', color='r')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, n_components+1))\n",
    "plt.axhline(y=0.9, color='k', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cumulative explained variance: {np.cumsum(pca_analysis.explained_variance_ratio_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with all features and metadata\n",
    "feature_dataset = pd.DataFrame(X_scaled)\n",
    "feature_dataset['emotion'] = metadata_df['emotion'].values\n",
    "feature_dataset['gender'] = metadata_df['gender'].values\n",
    "feature_dataset['intensity'] = metadata_df['intensity'].values\n",
    "\n",
    "# Let's look at the distribution of features by emotion\n",
    "# We'll examine the first few important features\n",
    "top_features = 5\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i in range(top_features):\n",
    "    plt.subplot(top_features, 1, i+1)\n",
    "    sns.boxplot(data=feature_dataset, x='emotion', y=i)\n",
    "    plt.title(f'Distribution of Feature {i+1} by Emotion')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel(f'Feature {i+1} Value')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Audio Feature Correlations\n",
    "\n",
    "Let's examine the correlations between our extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for the first 20 features\n",
    "n_features = min(20, X_scaled.shape[1])\n",
    "corr_matrix = pd.DataFrame(X_scaled[:, :n_features]).corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Complete Feature Dataset for Training\n",
    "\n",
    "Let's prepare the complete dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the complete feature dataset\n",
    "feature_dataset = pd.DataFrame(X_scaled)\n",
    "for col in ['actor_id', 'emotion', 'intensity', 'gender', 'sentence_id']:\n",
    "    if col in metadata_df.columns:\n",
    "        feature_dataset[col] = metadata_df[col].values\n",
    "\n",
    "# Save the dataset\n",
    "features_dir = os.path.join('..', 'data', 'features')\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "feature_dataset.to_pickle(os.path.join(features_dir, 'full_features.pkl'))\n",
    "\n",
    "print(f\"Saved feature dataset with shape {feature_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of the dataset\n",
    "feature_dataset[['emotion', 'intensity', 'gender']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Observations\n",
    "\n",
    "Based on our feature extraction and visualization, we can make several observations:\n",
    "\n",
    "1. **Preprocessing Effects**: Our preprocessing pipeline successfully normalizes the audio and reduces noise, resulting in cleaner waveforms.\n",
    "\n",
    "2. **Feature Differentiation**: The PCA and t-SNE visualizations show some clustering of emotions, indicating that our features contain information useful for emotion classification.\n",
    "\n",
    "3. **Feature Importance**: The first few principal components capture a significant portion of the variance in our dataset, suggesting that dimensionality reduction might be beneficial for model training.\n",
    "\n",
    "4. **Emotion Patterns**: Certain emotions like 'angry' and 'happy' appear to have distinct feature patterns compared to 'neutral' and 'sad' emotions.\n",
    "\n",
    "5. **Gender and Intensity Effects**: The visualizations suggest that gender and intensity level also influence the acoustic features, which could be important confounding variables to consider during model development.\n",
    "\n",
    "6. **Feature Correlations**: Many of our extracted features show high correlation, indicating redundancy in the feature set. Feature selection or dimensionality reduction will likely improve model performance.\n",
    "\n",
    "7. **Feature Distribution**: The boxplots show that some features have different distributions across emotions, which is promising for classification tasks.\n",
    "\n",
    "These insights will guide our approach to model development in the next phase of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Implement feature selection techniques to identify the most discriminative features\n",
    "2. Train baseline models (SVM, Random Forest, XGBoost) on the extracted features\n",
    "3. Perform hyperparameter optimization to improve model performance\n",
    "4. Evaluate models using cross-validation and various metrics\n",
    "5. Analyze model results to gain insights into emotion recognition patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
